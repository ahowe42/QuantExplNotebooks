{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TPOT - Python Automated Machine Learning Tool](https://epistasislab.github.io/tpot/)\n",
    "<img src=\"https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/tpot-ml-pipeline.png\" width=\"1000\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acronym \"TPOT\" stands for \"Tree-based Pipeline Optimization Tool\". TPOT was first introduced in the paper\n",
    "\n",
    "[Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science. Proceedings of GECCO 2016, pages 485-492.](https://dl.acm.org/doi/10.1145/2908812.2908918) - [which I have here]('./2016Olsonetal_EvaluationTPOT.pdf')\n",
    "\n",
    "TPOT uses [Genetic Programming](https://en.wikipedia.org/wiki/Genetic_programming), with a customizable objective function to identify an optimal pipeline of operations for:\n",
    "- feature engineering\n",
    "- feature selection\n",
    "- feature preprocessing\n",
    "- regression / classification modeling\n",
    "\n",
    "It works with scikit-learn, and is very customizable. At the end of it's search, TPOT can even generate a template python script to perform the best pipeline of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime as dat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some fake data with a known regressive structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' simulate some data '''\n",
    "# create the features & response\n",
    "p = 10\n",
    "n = 1000\n",
    "X = np.random.normal(loc=0, scale=1, size=(n,p))\n",
    "X[:,-1] = 0.42 + np.random.normal(loc=0, scale=0.0001, size=(n)) # replace with a very low variance feature\n",
    "B = [7, -6, 5, -4, 3] + [0]*(p-5)\n",
    "b = 0\n",
    "y = b + np.sum(B*X, axis=1) + np.random.normal(loc=0, scale=5, size=(n,))\n",
    "data = pd.DataFrame(data=X, columns=['X%d'%i for i in range(p)])\n",
    "features = data.columns.values.tolist()\n",
    "data['target'] = y\n",
    "data = data[['target'] + features]\n",
    "\n",
    "# talk\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize & Fit the TPOT Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define the allowed pipeline structure '''\n",
    "# define the type of oeprations allowed\n",
    "templt = 'Selector-Transformer-Regressor'\n",
    "\n",
    "# define common parameter choices\n",
    "lrnRat = [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
    "maxDep = [5, 10, None]\n",
    "tol = [1e-3, 1e-2, 1e-1]\n",
    "estim = [100]\n",
    "maxFeat = np.arange(0.05, 1.01, 0.10)\n",
    "minSplt = range(10, 21)\n",
    "minLeaf = range(10, 21)\n",
    "\n",
    "# define the config of allowable pipeline steps\n",
    "config = {\n",
    "    # feature selectors\n",
    "    'sklearn.feature_selection.SelectFwe': {\n",
    "        'alpha': np.arange(0, 0.05, 0.001),\n",
    "        'score_func': {'sklearn.feature_selection.f_regression':None}},\n",
    "    \n",
    "    'sklearn.feature_selection.SelectPercentile': {\n",
    "        'percentile': range(1, 100),\n",
    "        'score_func': {'sklearn.feature_selection.f_regression': None}},\n",
    "\n",
    "    'sklearn.feature_selection.VarianceThreshold': {\n",
    "        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]},\n",
    "\n",
    "    'sklearn.feature_selection.SelectFromModel': {\n",
    "        'threshold': np.arange(0, 1.01, 0.05),\n",
    "        'estimator': {'sklearn.ensemble.ExtraTreesRegressor': {'n_estimators': estim, 'max_features':maxFeat}}},\n",
    "    \n",
    "    # preprocessors\n",
    "    'sklearn.preprocessing.Binarizer': {\n",
    "        'threshold': np.arange(0.0, 1.01, 0.05)},\n",
    "\n",
    "    'sklearn.decomposition.FastICA': {\n",
    "        'tol': np.arange(0.0, 1.01, 0.05)},\n",
    "\n",
    "    'sklearn.preprocessing.MaxAbsScaler': {},\n",
    "\n",
    "    'sklearn.preprocessing.MinMaxScaler': {},\n",
    "\n",
    "    'sklearn.preprocessing.Normalizer': {\n",
    "        'norm': ['l1', 'l2', 'max']},\n",
    "\n",
    "    'sklearn.decomposition.PCA': {\n",
    "        'svd_solver': ['randomized'],\n",
    "        'iterated_power': range(1, 11)},\n",
    "\n",
    "    'sklearn.preprocessing.PolynomialFeatures': {\n",
    "        'degree': [2],\n",
    "        'include_bias': [False],\n",
    "        'interaction_only': [False]},\n",
    "\n",
    "    'sklearn.preprocessing.RobustScaler': {},\n",
    "\n",
    "    'sklearn.preprocessing.StandardScaler': {},\n",
    "\n",
    "    'tpot.builtins.ZeroCount': {},\n",
    "    \n",
    "    # regressors\n",
    "    'sklearn.linear_model.ElasticNetCV':{\n",
    "        'l1_ratio':np.arange(0.0, 1.01, 0.05),\n",
    "        'tol': tol},\n",
    "    \n",
    "    'sklearn.ensemble.GradientBoostingRegressor':{\n",
    "        'n_estimators': estim,\n",
    "        'loss': [\"ls\", \"lad\", \"huber\", \"quantile\"],\n",
    "        'learning_rate': lrnRat,\n",
    "        'max_depth': maxDep,\n",
    "        'min_samples_split': minSplt,\n",
    "        'min_samples_leaf': minLeaf,\n",
    "        'max_features': maxFeat,\n",
    "        'alpha': [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]},\n",
    "    \n",
    "    'sklearn.ensemble.AdaBoostRegressor':{\n",
    "        'n_estimators': [100],\n",
    "        'learning_rate': lrnRat,\n",
    "        'loss': [\"linear\", \"square\", \"exponential\"]},\n",
    "    \n",
    "    'sklearn.linear_model.LassoLarsCV': {\n",
    "        'normalize': [True, False]},\n",
    "        \n",
    "    'sklearn.ensemble.RandomForestRegressor': {\n",
    "        'n_estimators': estim,\n",
    "        'max_features': maxFeat,\n",
    "        'min_samples_split': minSplt,\n",
    "        'min_samples_leaf': minLeaf,\n",
    "        'bootstrap': [True, False]},\n",
    "        \n",
    "    'sklearn.linear_model.RidgeCV': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' run TPOT '''\n",
    "# set genetic algorithm parameters\n",
    "gens = 10    # number of generations\n",
    "early = 5   # number of generations with no improvement in objective function to terminate early\n",
    "pops = 20   # size of population\n",
    "mutat = 0.05 # mutation rate \n",
    "xover = 0.90 # crossover rate\n",
    "objective = 'neg_mean_squared_error' # objective function that TPOT will attempt to maxim\n",
    "randSeed = 42 # prng seed\n",
    "\n",
    "# generate for cross-validation\n",
    "nSplits = 10\n",
    "trainPerc = 0.7\n",
    "cvs = ShuffleSplit(nSplits, train_size=trainPerc)\n",
    "\n",
    "# init\n",
    "tReg = TPOTRegressor(generations=gens, population_size=pops, mutation_rate=mutat, crossover_rate=xover, scoring=objective,\n",
    "                     template=templt, config_dict=config, cv=cvs, use_dask=True, verbosity=3, n_jobs=-1, random_state=randSeed)\n",
    "\n",
    "# fit the regressor\n",
    "startTime = dat.datetime.now()\n",
    "tReg.fit(data[features].values, data.target.values)\n",
    "stopTime = dat.datetime.now()\n",
    "elapsTime = stopTime - startTime\n",
    "\n",
    "# talk\n",
    "print('Elapsed time = %s'%(stopTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes from Fit TPOT\n",
    "- TPOT will return the set of best models that score similarly but tradeoff on parameter complexity - this is called the [Pareto Front](https://en.wikipedia.org/wiki/Pareto_efficiency).\n",
    "- You can also access the best pipeline, which is an [sklearn pipeline object](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which we all know how to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can access *all* pipelines fit\n",
    "print('%d Pipelines Fit'%len(tReg.evaluated_individuals_.keys())) # note that this is equal to gens*pops\n",
    "# print an MST of text\n",
    "for (key, val) in tReg.evaluated_individuals_.items():\n",
    "    print('%0.3f - %s'%(val['internal_cv_score'], key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPOT provides a dict of pipelines on the Pareto Front\n",
    "print('Pareto Front')\n",
    "for key in tReg.pareto_front_fitted_pipelines_.keys():\n",
    "    print(key)\n",
    "\n",
    "# get the \"best\" pipeline\n",
    "print('Best Pipeline')\n",
    "bestPipe = tReg.fitted_pipeline_\n",
    "print(bestPipe)\n",
    "print('Best Pipeline RMSE = %0.3f'%math.sqrt(mean_squared_error(data.target.values, bestPipe.predict(data[features].values))))\n",
    "\n",
    "# get the estimator from the best pipeline\n",
    "bestEstim = bestPipe.steps[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at feature coefficients from the best pipeline\n",
    "coefs = pd.DataFrame(data=bestEstim.coef_, index=features, columns=['Coefficients'])\n",
    "coefs['abs'] = coefs.Coefficients.abs()\n",
    "coefs = coefs.sort_values(by=['abs'], ascending=False, inplace=False).drop(columns=['abs'], inplace=False)\n",
    "display(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Best Pipeline as Python Code\n",
    "This can be used as a template for starting more comprehensive modeling work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the resultant model\n",
    "pyFile = './TPOT_Demo.py'\n",
    "tReg.export(pyFile)\n",
    "\n",
    "# check it out\n",
    "print('Best TPOT Pipeline')\n",
    "with open(pyFile, 'rt') as f:\n",
    "    print(''.join(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
