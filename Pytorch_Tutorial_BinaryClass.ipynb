{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding & Optimizing Neural Networks with PyTorch\n",
    "## Binary Classification\n",
    "\n",
    "PyTorch is an optimized tensor library for modeling with deep neural network learning models. This notebook is adapted from my [prior work](https://github.com/ahowe42/QuantExplNotebooks/blob/master/src/my_neuralnetwork_pytorch.ipynb).\n",
    "\n",
    "Note that this is only my view of what a PyTorch tutorial should be, but there are many others. [The PyTorch Tutorials page](https://pytorch.org/tutorials/) is a good place to look.\n",
    "\n",
    "Of course, perusing [the docs](https://pytorch.org/docs/stable/index.html) is always a good idea.\n",
    "\n",
    "For deeper knowledge of how simple neural networks work, [this notebook](https://github.com/ahowe42/QuantExplNotebooks/blob/master/src/my_neuralnetwork.ipynb) may be a useful source. I created this notebook as part of my own learning process. In it, I coded the basic functionality of activated linear layers, forward propagation, loss computation, gradient calculation, and regularized backward propagation.\n",
    "\n",
    "- <a href=#DP>Data Preparation</a>\n",
    "- <a href=#DD>Dataset and DataLoader</a>\n",
    "- <a href=#NNA>Neural Network Architecture</a>\n",
    "- <a href=#NNO>Neural Network Optimization</a>\n",
    "- <a href=#LVP>Logging \\& Visualizing Progress</a>\n",
    "- <a href=#T>Training</a>\n",
    "- <a href=#S>PyTorch Sequential</a>\n",
    "- <a href=#PND>Parameterized Network Design</a>\n",
    "- <a href=#C>Checkpointing</a>\n",
    "- <a href=#E>Extensions</a>\n",
    "- <a href=#Bot>Go To Bottom</a>\n",
    "\n",
    "<a id=top></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipdb\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import chart_studio.plotly as ply\n",
    "import chart_studio.tools as plytool\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as plyoff\n",
    "\n",
    "plyoff.init_notebook_mode(connected=True)\n",
    "x1 = [1,4,7]; y1 = [7,5,7]\n",
    "x2 = [1,2,3,4,5,6,7]; y2 = [3,2,1,1,1,2,3]\n",
    "plyoff.iplot(go.Figure(data=[go.Scatter({'x':x1, 'y':y1, 'mode':'markers'}), go.Scatter({'x':x2, 'y':y2, 'mode':'lines'})],\n",
    "                       layout=go.Layout(autosize=False, width=400, showlegend=False,\n",
    "                                        xaxis={'showgrid':False, 'showticklabels':False},\n",
    "                                        title=\"Initialization Makes Me Smile<br>(and it's fun to show off a little...)\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "First, we generate some training / testing data with two features and a single binary response.\n",
    "\n",
    "<a id=DP></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' create simulated data '''\n",
    "# setup\n",
    "np.random.seed = 42\n",
    "n = 100\n",
    "p = 2\n",
    "trainPerc = 0.7\n",
    "\n",
    "# define features & response\n",
    "x = np.random.rand(n, p)\n",
    "print(x.shape)\n",
    "y = x[:,0]*5 + x[:,1]*3 + np.random.rand(n)\n",
    "y = np.atleast_2d(y > np.median(y)).T\n",
    "print(y.shape)\n",
    "\n",
    "# partition randomly into the training & testing sets\n",
    "ss = ShuffleSplit(n_splits=1, train_size=trainPerc)\n",
    "(trn, tst) = next(ss.split(x))\n",
    "trnX = x[trn,:]; trnY = y[trn,:]\n",
    "tstX = x[tst,:]; tstY = y[tst,:]\n",
    "\n",
    "# plot the data\n",
    "trnYFlat = np.squeeze(trnY); tstYFlat = np.squeeze(tstY); # squeee y for indexing into x\n",
    "trc = [go.Scatter({'x':trnX[trnYFlat,0], 'y':trnX[trnYFlat, 1], 'name':'Train 1 (> median)', 'mode':'markers',\n",
    "                   'marker':{'color':'red', 'symbol':'circle-open'}}, legendgroup='train'), \n",
    "        go.Scatter({'x':trnX[~trnYFlat,0], 'y':trnX[~trnYFlat, 1], 'name':'Train 0 (<= median)', 'mode':'markers',\n",
    "                   'marker':{'color':'blue', 'symbol':'square-open'}}, legendgroup='train'),\n",
    "        go.Scatter({'x':tstX[tstYFlat,0], 'y':tstX[tstYFlat, 1], 'name':'Test 1 (> median)', 'mode':'markers',\n",
    "                   'marker':{'color':'red', 'symbol':'circle-dot'}}, legendgroup='test'), \n",
    "        go.Scatter({'x':tstX[~tstYFlat,0], 'y':tstX[~tstYFlat, 1], 'name':'Test 0 (<= median)', 'mode':'markers',\n",
    "                   'marker':{'color':'blue', 'symbol':'square-dot'}}, legendgroup='test')]\n",
    "plyoff.iplot(go.Figure(data=trc, layout=go.Layout(title = 'Data')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "Data is accessed in a PyTorch neural network via a `DataLoader` object, which is an iterable over a `Dataset` object. Data needs to be defined as `Tensor` objects.\n",
    "\n",
    "### Tensor\n",
    "The `Tensor` object, representing a multi-dimensional matrix containing elements of a single data type, is the primary datatype in PyTorch. Common types include:\n",
    "- `IntTensor` - integers\n",
    "- `FloatTensor` - floats\n",
    "- `BoolTensor` - booleans\n",
    "\n",
    "All `Tensor` types can be seen [here](https://pytorch.org/docs/stable/tensors.html). Note that the response variable defined below is an integer, but defined as a generic `Tensor`. I defined it this way because the loss function throws an exception if it is an `IntTensor` or `BoolTensor`, which is odd.\n",
    "\n",
    "### Dataset\n",
    "For use cases where the data has features and responses, it's simplest if this has separate `x` and `y` (or perhaps `features` and `response`) `Tensor` objects. A [map-style Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is shown in this tutorial. These must overload the `__getitem__` and `__len__` methods. The former must return data for a given index key, and the latter is self-explanatory. PyTorch also has an [iterable-style Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset).\n",
    "\n",
    "### DataLoader\n",
    "The `DataLoader` object does just what it says - loads data. There are [many options](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for setting up a data loader, but the most commonly useful are probably:\n",
    "- `dataset` - This is required, obviously.\n",
    "- `batch_size` - This defaults to 1 if not specified. If minibatching is required (helps prevent optimizing getting stuck in a local optimum, as well as uses less memory), set this to the batch size, otherwise, set to the size of the dataset.\n",
    "- `shuffle` - This randomly permutes the order of the data with each epoch; the default value is False.\n",
    "\n",
    "In this tutorial, note that the training data is loaded in shuffled batches of size 5, but the testing data is all loaded as a single batch, and not shuffled.\n",
    "\n",
    "<a id=DD></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' setup for data access '''\n",
    "# now make the datasets & dataloaders\n",
    "batchSize = 5\n",
    "\n",
    "# Create the data class\n",
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.Tensor(y.astype(int))\n",
    "        self.len, self.p = self.x.shape\n",
    "    def __getitem__(self, index):      \n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# training data is accessed in batches, testing data is not\n",
    "trainData = Data(trnX, trnY)\n",
    "trainLoad = DataLoader(dataset=trainData, batch_size=batchSize, shuffle=True)\n",
    "testData = Data(tstX, tstY)\n",
    "testLoad = DataLoader(dataset=testData, batch_size=len(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "In PyTorch, the architecture of a simple NN is defined by a class that extends the `nn.Module` object. At a minimum, the `__init__` and `forward` methods must be overloaded. There is far more than I can say here that should be said here, so [RTFM](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A PyTorch NN is essentially a bunch of stuff around a list of PyTorch modules. These modules should be defined in the `__init__` method as an [nn.ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) object. This is an iterable container of Pytorch modules, which could be layers.\n",
    "\n",
    "There are several types of layers which can go into an NN, depending on the type of model desired, listed [here](https://pytorch.org/docs/stable/nn.html). This tutorial demonstrates a simple network with just activated `Linear` layers - $y_i = f\\left(a\\times x_i + b\\right)$. The `Linear` layer must be defined with an input and output size. Note that each `Linear` layer has the `bias` and `weight` attributes. The biases are typically initialized to 0. There are several choices for initialization of weights in the `nn` module. In this tutorial, we have the options:\n",
    "- uniform - Uniform(0, 1)\n",
    "- xavier uniform - Glorot Initialization\n",
    "- kaiming uniform - He Initialization\n",
    "\n",
    "After all the 4 hardcoded layers have been defined and initialized, they're added to the list of modules. The `__init__` method also defines the `Dropout` layer and the activation function for each layer (sans the input layer). In PyTorch, choices for the activation functions are defined in [nn.functional](https://pytorch.org/docs/stable/nn.functional.html). The [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html) (rectified linear unit) or [leaky ReLU](https://pytorch.org/docs/stable/generated/torch.nn.functional.leaky_relu.html) are the most common activation functions for hidden layers, defined as \n",
    "\\begin{equation}\n",
    "f(x_i) = \\begin{cases}\n",
    "0 & x_i < 0\\\\\n",
    "x_i & x_i >= 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "f(x_i, s) = \\begin{cases}\n",
    "x_i\\times-s & x_i < 0\\\\\n",
    "x_i & x_i >= 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "The leaky ReLU multiplicatively adds a slight negative slope $s$ for negative input values.\n",
    "\n",
    "For classification problems, the [sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) activation function $f(x_i) = \\left(1+e^{-x_i}\\right)^{-1}$, which scales the input values to the [0, 1] range, is most commonly used. In the case of binary classification, the value for $f(x_i)$ is then interpreted as the probability that $y_i$ is the target (True) class.\n",
    "\n",
    "### Forward Propagation\n",
    "Forward propagation needs to be implemented in a class method named `forward`, which takes the input data values as an argument. This is the process of pushing input data through the network layers to compute a prediction. In this tutorial, forward propagation essentially computes this:\n",
    "\\begin{equation}\n",
    "\\hat{y}_i = Sigmoid\\left(A_4\\times ReLU\\left(A_3\\times ReLU\\left(A_2\\times ReLU\\left(A_1\\times x_i+b_1\\right) + b_2\\right) + b_3\\right) + b_4\\right)\n",
    "\\end{equation}\n",
    "Forward propagation is where any kind of special layers would be applied - such as dropout or batch normalization. Dropout is implemented as a [Dropout layer](https://pytorch.org/docs/stable/generated/torch.nn.Dropout) parameterized by a probability. The `Dropout` layer randomly set observations in the input data to 0 each time it's called. This helps in regularization and to prevent the network getting stuck in a local optimum. I believe the network could be designed with an individual `Dropout` layer before each `Linear` layer, but there are no parameters for backward propagation, and it would add unnecessary complexity. Thus, the network below has a single `Dropout` layer defined in the `__init__` method that is repeatedly called in the `forward` method.\n",
    "\n",
    "When trainig the network, the `forward` method is not explicitly called. Forward propagation is execute by simply calling the network object, passing the input features as arguments, as in `myNN(x)`.\n",
    "\n",
    "### Backward Propagation\n",
    "Backward propagation is the process by which errors as computed by the loss function are propagated backwards through the network, using the gradients (first derivative) of the layers' parameters. Backward propagation is performed by the loss and optimization functions together.\n",
    "\n",
    "<a id=NNA></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define the model class for a neural net with hidden layers & dropout '''\n",
    "class myNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNN, self).__init__()\n",
    "        # define the activations\n",
    "        self.activations = ['relu', 'relu', 'relu', 'sigmoid']\n",
    "        # define the linear layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        lin1 = nn.Linear(p, 10)\n",
    "        lin2 = nn.Linear(10, 20)\n",
    "        lin3 = nn.Linear(20, 10)\n",
    "        lin4 = nn.Linear(10, 1)\n",
    "        # initialize the biases\n",
    "        nn.init.zeros_(lin1.bias)\n",
    "        nn.init.zeros_(lin2.bias)\n",
    "        nn.init.zeros_(lin3.bias)\n",
    "        nn.init.zeros_(lin4.bias)\n",
    "        # initialize the weights\n",
    "        self.weightInit = 'kai'\n",
    "        if self.weightInit == 'uni':\n",
    "            nn.init.uniform_(lin1.weight)\n",
    "            nn.init.uniform_(lin2.weight)\n",
    "            nn.init.uniform_(lin3.weight)\n",
    "            nn.init.uniform_(lin4.weight)\n",
    "        elif self.weightInit == 'xav':\n",
    "            nn.init.xavier_uniform_(lin1.weight)\n",
    "            nn.init.xavier_uniform_(lin2.weight)\n",
    "            nn.init.xavier_uniform_(lin3.weight)\n",
    "            nn.init.xavier_uniform_(lin4.weight)\n",
    "        elif self.weightInit == 'kai':\n",
    "            nn.init.kaiming_uniform_(lin1.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(lin2.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(lin3.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(lin4.weight, nonlinearity='relu')\n",
    "        # add the layers to the model\n",
    "        self.linears.append(lin1)\n",
    "        self.linears.append(lin2)\n",
    "        self.linears.append(lin3)\n",
    "        self.linears.append(lin4)\n",
    "        # define last stuff\n",
    "        self.len = 4\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (L, A) in enumerate(zip(self.linears, self.activations)):\n",
    "            # dropout if not the output layer\n",
    "            if i < self.len - 1:\n",
    "                x = self.drop(L(x))\n",
    "            else:\n",
    "                x = L(x)\n",
    "            # compute the activation\n",
    "            if A == 'relu':\n",
    "                x = nn.functional.relu(x)\n",
    "            elif A == 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif A == 'leakyrelu':\n",
    "                x = nn.functional.leaky_relu(x)\n",
    "            elif A == 'tanh':\n",
    "                x = torch.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    def __str__(self):\n",
    "        mn = super(myNN, self).__str__()\n",
    "        return '%s\\nActivations: %s'%(mn, self.activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Optimization\n",
    "In addition to the network architecture, a NN model definition includes an optimizer, it's parameters, and a loss function.\n",
    "\n",
    "### Optimization Method\n",
    "The [opim module](https://pytorch.org/docs/stable/optim.html) includes several common optimization algorithms, including:\n",
    "- Adaptive methods (Adam, Adagrad, Adamax)\n",
    "- RMSprop\n",
    "- [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) - this is the optimzer used in this tutorial\n",
    "\n",
    "Each optimizer method needs to have the network's parameters to be optimized, the learning rate for backward propagation, and any method-specific parameters. There is no need to manually specify the parameters to be optimized, as the `nn.Module` object has an inheritable `parameters` method which returns a python generator to iterate over the network parameters. The learning rate typically reduces the amount by which prediction errors are propagated back through the network, which helps the NN to converge. A higher learning rate is often good earlier in training, while a lower learning rate can be better towards the end. PyTorch includes in the optim module several schedulers for decaying the learning rate as `lr_scheduler` objects. In this tutorial, a [multiplicative step scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html) is used, which reduces the learning rate from the initial value (0.1) by a specified proportion (50%), after each multiple of a specified number of epochs (50).\n",
    "\n",
    "### Loss Function\n",
    "PyTorch includes several loss functions, listed in the [nn module](https://pytorch.org/docs/stable/nn.html). As this tutorial applies a simple neural network for a binary classification problem, the [binary cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) is used. For each observation, the loss function as implemented here computes\n",
    "\\begin{equation}\n",
    "l\\left(y_i, \\hat{y}_i\\right) = y_i \\times \\log\\hat{y}_i + \\left(1-y_i\\right)\\times \\log\\left(\\hat{y}_i\\right)\n",
    "\\end{equation}\n",
    "Optional arguments to the `BCELoss` function include observation weights, and either a sum or mean reduction.\n",
    "\n",
    "### Backward Propagation\n",
    "Backward propagation is the process by which errors as computed by the loss function are propagated backwards through the network, using the gradients (first derivative) of the layers' parameters. All loss functions have a `backward` method; despite it's name, it does not actually propagate the errors backward through the network, it simply computes the gradients and saves them in the parameters' `grad` attributes. Because the network architecture is built up by PyTorch layers and activation functions, it already knows how to do this. The [autograd](https://pytorch.org/docs/stable/autograd.html) module is responsible for this automatic differentiation. Backward propagation of the errors is actually performed by the optimizer.\n",
    "\n",
    "<a id=NNO></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define the model & operating parameters '''\n",
    "# set the learning rate\n",
    "learningRate = 0.1\n",
    "stepSize = 50\n",
    "stepMult = 0.5\n",
    "\n",
    "# setup the network\n",
    "torch.manual_seed(42)\n",
    "classificationNN = myNN()\n",
    "print(classificationNN)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = torch.optim.SGD(params=classificationNN.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=stepSize, gamma=stepMult)\n",
    "                                \n",
    "# setup the loss function\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging & Visualizing Progress\n",
    "In addition to the logging & visualizing results and progress that we can do ourselves with just python, PyTorch has the ability to log results to [TensorBoard](https://pytorch.org/docs/stable/tensorboard.html), which comes from TensorFlow. There are a lot of options, but the simplest is to create a `SummaryWriter` object, potentially specifying the directory, a comment, and file suffixes. If a directory is not specified, all output goes to *./runs/*. Scalar values can be added to Tensorboard with the `add_scalar` method. The name of what's being added and the epoch must be specified, in addition to the value. In addition to scalar values, there are several other options for adding data to TensorBoard. The `add_graph` method may be used to add an interactive visual of the NN's execution graph; this can be done by passing the network object as the method's `model` argument.\n",
    "\n",
    "The data being logged can be viewed by executing *tensorboard --logdir=logging directory* on the command line, with the logging directory what is passed to the `SummaryWriter` object in the `log_dir` argument.\n",
    "\n",
    "<a id=LVP></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tensorboard writer\n",
    "writr = SummaryWriter(log_dir='./pytorch_tutorial/hardcode', comment='Hardcoded NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NN by Iterating over Epochs.\n",
    "Each epoch has a training and evaluation phase, though the latter is not strictly needed.\n",
    "\n",
    "### Training\n",
    "The training phase is started by executing the network's `train(True)` method, inherited from the base class. This tells PyTorch that operations like dropout or batch normalization shoud occur, which wouldn't occur during evaluation. Then, for each minibatch:\n",
    "1. training data is propagated forward through the network\n",
    "2. the loss for the training data is computed\n",
    "3. errors are propagated back through the network, updaing the parameters\n",
    "\n",
    "Before executing the `backward` method of the loss function, the optimizer's `zero_grad` method should be executed. This sets the parameter gradients to 0, so they don't accumulate across batches. Then, the optimizer's `step` method should be executed. This actually propagates the errors back through the network. Finally, if a decaying learning rate is used, the scheduler's `step` method should be called after iterating over all minibatches.\n",
    "\n",
    "### Evaluation\n",
    "The evaluation phase is where the model's performance - loss and / or error - can be computed on the test dataset, or on the entire training dataset if it wasn't saved during the training phase. Evaluation code should all be run within the context of a `with torch.no_grad()` block, which prevents any gradient computations. The network's `train()` method should be called with `False` as the argument, which is equivalent to executing it's `eval` method. This turns off the `Dropout` layer.\n",
    "\n",
    "Note that the predicted values from executing the network's `forward` step are probabilities (thanks to the sigmoid activation function) stored in a PyTorch tensor. To compute anything with these results - such as accuracy, it's usually best to convert these to a numpy array or something else. This can be done by executing `.detach().numpy()` on the tensor, which copies the data into a new tensor which is not part of the calculation graph (hence, it's *detached*), then converts it. We want to compute the binary classification accuracy, but $\\hat{y}_i$ is a probability, so we need to convert the probabilities to binary flags. We do this using a simple 50% threshold $P_{thresh}$:\n",
    "\\begin{equation}\n",
    "\\hat{y}_i = \\begin{cases}\n",
    "\\hat{y}_i > P_{thresh} & 1\\\\\n",
    "\\hat{y}_i <= P_{thresh} & 0\n",
    "\\end{cases}.\n",
    "\\end{equation}\n",
    "\n",
    "As an aside, this is exactly what happens with logistic regression:\n",
    "\\begin{align}\n",
    "\\text{log_odds}_i(y_i=1)=&\\frac{P\\left(y_i=1\\right)}{1-P\\left(y_i=1\\right)} = b_0+\\sum_{i=1}^pb_ix_i^j,\\ j=1,\\ldots,p\\\\\n",
    "P\\left(y_i=1\\right) =& \\left(1+e^{-\\text{log_odds}_i}\\right)^{-1}\\\\\n",
    "\\hat{y}_i=&\\begin{cases}1 & P\\left(y_i=1\\right)>P_{thresh}\\\\\n",
    "0&P\\left(y_i=1\\right)<=P_{thresh}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<a id=T></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(data, network, loss, optimizer, scheduler, epochs, writer, randState=None, talkFreq=0.2):\n",
    "    '''\n",
    "    Train a neural network specified by a network, optimizer, and loss, fitting to data from\n",
    "    a training dataloader, and evaluating on a testing dataloader. NB This procedure updates\n",
    "    the input network *in place*.\n",
    "    :param data: list holding the training set data loader and testing set dataloader. The first\n",
    "        may load in batches, while the second may not\n",
    "    :param network: PyTorch NN architecture as defined to extend the nn.Module class, or using\n",
    "        the Sequential constructor\n",
    "    :param loss: PyTorch loss function\n",
    "    :param optimizer: PyTorch optimizer\n",
    "    :param scheduler: PyTorch learning rate decay scheduler\n",
    "    :param epochs: integer number of epochs\n",
    "    :param writer: PyTorch TensorBoard SummaryWriter object\n",
    "    :param randState: optional seed for PyTorch psuedo random number generator\n",
    "    :param talkFreq: optional (default=0.2) frequency with which progress should be printed\n",
    "    :return trnLoss: loss on training set per epoch\n",
    "    :return trnAcc: accuracy on training set per epoch\n",
    "    :return tstLoss: loss on testing set per epoch\n",
    "    :return tstAcc: accuracy on testing set per epoch\n",
    "    '''\n",
    "    \n",
    "    # set the prng seed, maybe\n",
    "    if randState:\n",
    "        torch.manual_seed(randState)\n",
    "        writer.add_scalar('Params/random_state', randState, 0)\n",
    "        \n",
    "    # save the initial learning rate\n",
    "    writer.add_scalar('Params/initial_learning_rate', optimizer.param_groups[0]['initial_lr'], 0)\n",
    "    \n",
    "    # add the model graph\n",
    "    try:\n",
    "        writer.add_graph(network, data[0].dataset.x)\n",
    "    except TypeError as err:\n",
    "        print(\"Network may be from nn.Sequential, so can't be added to TensorBoard!\")\n",
    "        \n",
    "    # get the data loaders\n",
    "    trn, tst = data\n",
    "    \n",
    "    # containers for training / testing loss & accuracy\n",
    "    trnLoss = [np.inf]*epochs\n",
    "    trnAcc = [0]*epochs\n",
    "    tstLoss = [np.inf]*epochs\n",
    "    tstAcc = [0]*epochs\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # train with minibatch gradient descent\n",
    "        network.train(True) # setting train to True tells pytorch that ops like dropout / batch normalization to occur, which wouldn't occur during evaluation\n",
    "        # iterate over batches\n",
    "        for indx, (x, y) in enumerate(trn):\n",
    "            # forward step\n",
    "            yhat = network(x) # implements forward propagation as implemented by the forward() method\n",
    "            # compute loss (not storing for now, will do after minibatching)\n",
    "            l = loss(yhat, y)\n",
    "            # backward step\n",
    "            optimizer.zero_grad() # set gradients to zero before backprop, so there's no accumulation among batches\n",
    "            l.backward() # backward propagation computes the gradients of the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "        # update the learning rate\n",
    "        scheduler.step()\n",
    "        writer.add_scalar('Params/learning_rate', scheduler.get_last_lr()[0], epoch)\n",
    "        \n",
    "        # evaluate performance\n",
    "        with torch.no_grad():\n",
    "            network.train(False) # using no_grad() and train() false turns off any gradient updating or special functionality\n",
    "            # evaluate loss & accuracy on training set\n",
    "            yhat = network(trn.dataset.x)\n",
    "            trnLoss[epoch] = loss(yhat, trn.dataset.y)\n",
    "            trnAcc[epoch] = accuracy_score(trn.dataset.y.numpy(), yhat.detach().numpy()>0.5)           \n",
    "            # evaluate loss & accuracy on testing set\n",
    "            yhat = network(tst.dataset.x)\n",
    "            tstLoss[epoch] = loss(yhat, tst.dataset.y)\n",
    "            tstAcc[epoch] = accuracy_score(tst.dataset.y.numpy(), yhat.detach().numpy()>0.5)\n",
    "            # tensorboard\n",
    "            writer.add_scalar('Train/Loss', trnLoss[epoch], epoch)\n",
    "            writer.add_scalar('Test/Loss', tstLoss[epoch], epoch)\n",
    "            writer.add_scalar('Train/Accuracy', trnAcc[epoch], epoch)\n",
    "            writer.add_scalar('Test/Accuracy', tstAcc[epoch], epoch)\n",
    "            # maybe talk\n",
    "            if epoch % (epochs*talkFreq) == 0:\n",
    "                print('Epoch %d Training (Testing) Loss = %0.2f (%0.2f), & Accuracy = %0.2f (%0.2f)'%\n",
    "                      (epoch, trnLoss[epoch], tstLoss[epoch], trnAcc[epoch], tstAcc[epoch]))\n",
    "\n",
    "    print('==========\\nTraining Initial Loss (Accuracy) = %0.2f (%0.2f), Final Loss (Accuracy) = %0.2f (%0.2f)'%\\\n",
    "          (trnLoss[0], trnAcc[0], trnLoss[-1], trnAcc[-1]))\n",
    "    print('Testing Initial Loss (Accuracy) = %0.2f (%0.2f), Final Loss (Accuracy) = %0.2f (%0.2f)'%\\\n",
    "          (tstLoss[0], tstAcc[0], tstLoss[-1], tstAcc[-1]))\n",
    "    \n",
    "    # return results\n",
    "    return trnLoss, trnAcc, tstLoss, tstAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "randState = 42\n",
    "epochs = 500\n",
    "talkFreq = 0.2\n",
    "\n",
    "trnLoss, trnAcc, tstLoss, tstAcc = trainNN([trainLoad, testLoad], classificationNN, loss, optimizer, scheduler, epochs, writr, randState, talkFreq)\n",
    "\n",
    "# visualize loss & accuracy progressions\n",
    "x = list(range(epochs))\n",
    "lossesAccs = np.asarray([(rl.item(), tl.item(), ra.item(), ta.item()) for (rl, tl, ra, ta)\n",
    "                         in zip(trnLoss, tstLoss, trnAcc, tstAcc)])\n",
    "\n",
    "trc = [go.Scatter({'x':x, 'y':lossesAccs[:,0], 'name':'Train Loss', 'mode':'lines',\n",
    "                   'line':{'color':'red', 'dash':'dash'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,1], 'name':'Test Loss', 'mode':'lines',\n",
    "                   'line':{'color':'red'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,2], 'name':'Train Acc.', 'mode':'lines',\n",
    "                   'line':{'color':'green', 'dash':'dash'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,3], 'name':'Test Acc', 'mode':'lines',\n",
    "                   'line':{'color':'green'}})]\n",
    "lout = go.Layout(title='Modeling Results; Testing Accuracy = %0.2f%%'%(100*tstAcc[-1]), height=400, legend={'orientation':'h', 'xanchor':'center', 'yanchor':'top', 'y':1.20, 'x':0.5})\n",
    "plyoff.iplot(go.Figure(data=trc, layout=lout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Sequential\n",
    "So far in this tutorial, we've seen an NN with the layers hardcoded line-by-line, then added to the module list. PyTorch provides the [Sequential function](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to clean this up, and provide a few more benefits. The previously-seen NN coded the network architecture as essentially a module list + forward propagation method. In contrast, the Sequential function chains together a series of modules - like a scikit-learn pipeline - such that no separate `forward` method is needed. Data passed into the returned NN is automatically passed along the member modules in sequence. Hence the name.\n",
    "\n",
    "Note that `Sequential` returns a PyTorch `nn.modules.container.Sequential` object. Because this is not an `nn.module` object, the `add_graph` TensorBoard method will **throw a type exception**. I have not seen a way around this.\n",
    "\n",
    "<a id=S></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define a model, using Sequential '''\n",
    "# set the learning rate\n",
    "learningRate = 0.1\n",
    "stepSize = 50\n",
    "stepMult = 0.5\n",
    "\n",
    "# define the network\n",
    "torch.manual_seed(42)\n",
    "classificationNN = nn.Sequential(\n",
    "    nn.Linear(p, 10), nn.Dropout(0.2), nn.ReLU(),\n",
    "    nn.Linear(10, 20), nn.Dropout(0.2), nn.ReLU(),\n",
    "    nn.Linear(20, 10), nn.Dropout(0.2), nn.ReLU(),\n",
    "    nn.Linear(10, 1), nn.Sigmoid()\n",
    ")\n",
    "print(classificationNN)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = torch.optim.SGD(params=classificationNN.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=stepSize, gamma=stepMult)\n",
    "                                \n",
    "# setup the loss function\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# define the tensorboard writer\n",
    "writr = SummaryWriter(log_dir='./pytorch_tutorial/sequential/', comment='Sequential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "randState = 42\n",
    "epochs = 500\n",
    "talkFreq = 0.2\n",
    "\n",
    "trnLoss, trnAcc, tstLoss, tstAcc = trainNN([trainLoad, testLoad], classificationNN, loss, optimizer, scheduler, epochs, writr, randState, talkFreq)\n",
    "\n",
    "# visualize loss & accuracy progressions\n",
    "x = list(range(epochs))\n",
    "lossesAccs = np.asarray([(rl.item(), tl.item(), ra.item(), ta.item()) for (rl, tl, ra, ta)\n",
    "                         in zip(trnLoss, tstLoss, trnAcc, tstAcc)])\n",
    "\n",
    "trc = [go.Scatter({'x':x, 'y':lossesAccs[:,0], 'name':'Train Loss', 'mode':'lines',\n",
    "                   'line':{'color':'red', 'dash':'dash'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,1], 'name':'Test Loss', 'mode':'lines',\n",
    "                   'line':{'color':'red'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,2], 'name':'Train Acc.', 'mode':'lines',\n",
    "                   'line':{'color':'green', 'dash':'dash'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,3], 'name':'Test Acc', 'mode':'lines',\n",
    "                   'line':{'color':'green'}})]\n",
    "lout = go.Layout(title='Modeling Results; Testing Accuracy = %0.2f%%'%(100*tstAcc[-1]), height=400, legend={'orientation':'h', 'xanchor':'center', 'yanchor':'top', 'y':1.20, 'x':0.5})\n",
    "plyoff.iplot(go.Figure(data=trc, layout=lout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized Network Design\n",
    "Both networks designed thus far are hardcoded, which is not ideal. Finally, let's see creating the same network without hardcoding:\n",
    "- number of layers\n",
    "- layer sizes\n",
    "- weights initialization method\n",
    "- activation functions\n",
    "- dropout probability\n",
    "\n",
    "<a id=PND></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define the model class for a neural net with variable hidden layers & dropout '''\n",
    "class myNN(nn.Module):\n",
    "    def __init__(self, layerNodes, wInit, pDropout, activations):\n",
    "        super(myNN, self).__init__()\n",
    "        self.activations = activations[1:]\n",
    "        self.len = len(layerNodes)-1\n",
    "        self.drop = nn.Dropout(pDropout)\n",
    "        self.linears = nn.ModuleList()\n",
    "        for I, O in zip(layerNodes, layerNodes[1:]):\n",
    "            # create the layer\n",
    "            lin = nn.Linear(I, O)\n",
    "            # initialize it\n",
    "            nn.init.zeros_(lin.bias)\n",
    "            if wInit == 'uni':\n",
    "                nn.init.uniform_(lin.weight)\n",
    "            elif wInit == 'xav':\n",
    "                nn.init.xavier_uniform_(lin.weight)\n",
    "            elif wInit == 'kai':\n",
    "                nn.init.kaiming_uniform_(lin.weight, nonlinearity='relu')\n",
    "            # and now add it\n",
    "            self.linears.append(lin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (L, A) in enumerate(zip(self.linears, self.activations)):\n",
    "            # dropout if not the output layer\n",
    "            if i < self.len - 1:\n",
    "                x = self.drop(L(x))\n",
    "            else:\n",
    "                x = L(x)\n",
    "            # compute the activation\n",
    "            if A == 'relu':\n",
    "                x = nn.functional.relu(x)\n",
    "            elif A == 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif A == 'leakyrelu':\n",
    "                x = nn.functional.leaky_relu(x)\n",
    "            elif A == 'tanh':\n",
    "                x = torch.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    def __str__(self):\n",
    "        mn = super(myNN, self).__str__()\n",
    "        return '%s\\nActivations: %s'%(mn, self.activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then setup things like this. The parmameterized construction makes it easy to try different network architectures by just editing the arguments to the network constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define the model & operating parameters '''\n",
    "# define the modeling parameters\n",
    "layers = (p, 10, 20, 10, 1) # input layer size, hidden layer sizes, output layer size\n",
    "activations = (None, 'relu', 'relu', 'relu', 'sigmoid') # first element should be None; it's for the input layer\n",
    "pDropout = 0.2\n",
    "weightInit = 'kai'\n",
    "\n",
    "# set the learning rate\n",
    "learningRate = 0.1\n",
    "stepSize = 50\n",
    "stepMult = 0.5\n",
    "\n",
    "# setup the network\n",
    "torch.manual_seed(42)\n",
    "classificationNN = myNN(layers, weightInit, pDropout, activations)\n",
    "print(classificationNN)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = torch.optim.SGD(params=classificationNN.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=stepSize, gamma=stepMult)\n",
    "                                \n",
    "# setup the loss function\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# define the tensorboard writer\n",
    "writr = SummaryWriter(log_dir='./pytorch_tutorial/parameterized', comment='Parameterized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "randState = 42\n",
    "epochs = 500\n",
    "talkFreq = 0.2\n",
    "\n",
    "trnLoss, trnAcc, tstLoss, tstAcc = trainNN([trainLoad, testLoad], classificationNN, loss, optimizer, scheduler, epochs, writr, randState, talkFreq)\n",
    "\n",
    "# visualize loss & accuracy progressions\n",
    "x = list(range(epochs))\n",
    "lossesAccs = np.asarray([(rl.item(), tl.item(), ra.item(), ta.item()) for (rl, tl, ra, ta)\n",
    "                         in zip(trnLoss, tstLoss, trnAcc, tstAcc)])\n",
    "\n",
    "trc = [go.Scatter({'x':x, 'y':lossesAccs[:,0], 'name':'Train Loss', 'mode':'lines',\n",
    "                   'line':{'color':'red', 'dash':'dash'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,1], 'name':'Test Loss', 'mode':'lines',\n",
    "                   'line':{'color':'red'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,2], 'name':'Train Acc.', 'mode':'lines',\n",
    "                   'line':{'color':'green', 'dash':'dash'}}),\n",
    "       go.Scatter({'x':x, 'y':lossesAccs[:,3], 'name':'Test Acc', 'mode':'lines',\n",
    "                   'line':{'color':'green'}})]\n",
    "lout = go.Layout(title='Modeling Results; Testing Accuracy = %0.2f%%'%(100*tstAcc[-1]), height=400, legend={'orientation':'h', 'xanchor':'center', 'yanchor':'top', 'y':1.20, 'x':0.5})\n",
    "plyoff.iplot(go.Figure(data=trc, layout=lout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "[This](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) page shows how to checkpoint training progress on a neural network model. This can be useful for backing up in-process model training, especially when training is computationally-intensive and takes a substantial amount of team. It can also be useful for training a model, then continuing training later on with more data, or if more iteration epochs are desired.\n",
    "\n",
    "In short, this involves two simple steps:\n",
    "1. serialize the `state_dict` from both the NN and the optimizer - can use `torch.save` on a dictionary of content\n",
    "2. later on, reinstantiate the NN and optimizer, then use their `load_state_dict` methods to reload the serialized states\n",
    "\n",
    "<a id=C></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "This tutorial demonstrated how to build a simple neural network model for binary classification. Modifying this for multinomial classification or for a regression problem would be straightforward. There are many other options and functionalities in PyTorch that have not been explored here, but you should now be able to understand how / where to implement them as needed. More complex deep learning networks - such as [Recurrent Neural Networks (RNNS)](https://en.wikipedia.org/wiki/Recurrent_neural_network), [Convolutional Neural Networks (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network), etc would be built similarly, but with different layers and operations.\n",
    "\n",
    "See below for a sample Convolutional Recurrent Neural Network, **but note that the subsequent cell will not execute**. This is actual code from the Iron Ore Price Prediction Project.\n",
    "\n",
    "<a id=E></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Recurrent Neural Network\n",
    "    https://arxiv.org/pdf/1907.04155.pdf\n",
    "    https://arxiv.org/pdf/1506.04214.pdf\n",
    "    \"\"\"\n",
    "    layers = {}\n",
    "    def __init__(self, input_dim, n_time_steps, output_dim, kernel_size1=7, kernel_size2=5, kernel_size3=3,\n",
    "                 n_channels1=32, n_channels2=32, n_channels3=32, n_units1=32, n_units2=32, n_units3=32):\n",
    "        \"\"\" initializer - create the layers \"\"\"\n",
    "        super().__init__()\n",
    "        # average pooling layers\n",
    "        self.avg_pool1 = nn.AvgPool1d(2, 2)\n",
    "        self.avg_pool2 = nn.AvgPool1d(4, 4)\n",
    "        # convolutional -> convoluional -> recurrent -> padding layers\n",
    "        self.conv11 = nn.Conv1d(input_dim, n_channels1, kernel_size=kernel_size1)\n",
    "        self.conv12 = nn.Conv1d(n_channels1, n_channels1, kernel_size=kernel_size1)\n",
    "        self.gru1 = nn.GRU(n_channels1, n_units1, batch_first=True)\n",
    "        self.zp11 = nn.ConstantPad1d(((kernel_size1 - 1), 0), 0)\n",
    "        self.zp12 = nn.ConstantPad1d(((kernel_size1 - 1), 0), 0)\n",
    "        # convolutional -> convoluional -> recurrent -> padding layers\n",
    "        self.conv21 = nn.Conv1d(input_dim, n_channels2, kernel_size=kernel_size2)\n",
    "        self.conv22 = nn.Conv1d(n_channels2, n_channels2, kernel_size=kernel_size2)\n",
    "        self.gru2 = nn.GRU(n_channels2, n_units2, batch_first=True)\n",
    "        self.zp21 = nn.ConstantPad1d(((kernel_size2 - 1), 0), 0)\n",
    "        self.zp22 = nn.ConstantPad1d(((kernel_size2 - 1), 0), 0)\n",
    "        # convolutional -> convoluional -> recurrent -> padding layers\n",
    "        self.conv31 = nn.Conv1d(input_dim, n_channels3, kernel_size=kernel_size3)\n",
    "        self.conv32 = nn.Conv1d(n_channels3, n_channels3, kernel_size=kernel_size3)\n",
    "        self.gru3 = nn.GRU(n_channels3, n_units3, batch_first=True)\n",
    "        self.zp31 = nn.ConstantPad1d(((kernel_size3 - 1), 0), 0)\n",
    "        self.zp32 = nn.ConstantPad1d(((kernel_size3 - 1), 0), 0)\n",
    "        # linear output layers\n",
    "        self.linear1 = nn.Linear(n_units1 + n_units2 + n_units3, output_dim)\n",
    "        self.linear2 = nn.Linear(input_dim * n_time_steps, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' forward propagation of input data through the network '''\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # line1\n",
    "        y1 = self.zp11(x)\n",
    "        y1 = torch.relu(self.conv11(y1))\n",
    "        y1 = self.zp12(y1)\n",
    "        y1 = torch.relu(self.conv12(y1))\n",
    "        y1 = y1.permute(0, 2, 1)\n",
    "        out, h1 = self.gru1(y1)\n",
    "        # line2\n",
    "        y2 = self.avg_pool1(x)\n",
    "        y2 = self.zp21(y2)\n",
    "        y2 = torch.relu(self.conv21(y2))\n",
    "        y2 = self.zp22(y2)\n",
    "        y2 = torch.relu(self.conv22(y2))\n",
    "        y2 = y2.permute(0, 2, 1)\n",
    "        out, h2 = self.gru2(y2)\n",
    "        # line3\n",
    "        y3 = self.avg_pool2(x)\n",
    "        y3 = self.zp31(y3)\n",
    "        y3 = torch.relu(self.conv31(y3))\n",
    "        y3 = self.zp32(y3)\n",
    "        y3 = torch.relu(self.conv32(y3))\n",
    "        y3 = y3.permute(0, 2, 1)\n",
    "        out, h3 = self.gru3(y3)\n",
    "        h = torch.cat([h1[-1], h2[-1], h3[-1]], dim=1)\n",
    "        out1 = self.linear1(h)\n",
    "        out2 = self.linear2(x.contiguous().view(x.shape[0], -1))\n",
    "        out = out1 + out2\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define the model & operating parameters '''\n",
    "network = ConvRNN(input_dim=feats.shape[2], n_time_steps=feats.shape[1], output_dim=respDays,\n",
    "                  n_channels1=channels[0], n_channels2=channels[1], n_channels3=channels[2],\n",
    "                  n_units1=units[0], n_units2=units[1], n_units3=units[2])\n",
    "opt = torch.optim.Adam(model.parameters(), lr=modelParams['learningRate'])\n",
    "epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, modelParams['learnRateDecayStep'], gamma=modelParams['learnRateMDecay'])\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End, Happy Torching!\n",
    "<font size='1'>(wait, what?)</font>\n",
    "\n",
    "<a id=Bot></a>\n",
    "<a href=#top>Go To Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
